---
title: "Projekt_CV"
format:
  html:
    embed-resources: true
editor: visual
---

```{r message=FALSE, warning=FALSE, echo=FALSE}
library(tidyverse)
library(keras)
```

## Tematyka zadania

Celem tego projektu jest zbudowanie modelu konwolucyjnej sieci neuronowej służącej do klasyfikacji dzieł sztuki ze względu na gatunek (11 etykiet) oraz stylów malarskich (27 etykiet).

Zbiór danych którym posłużyliśmy się do trenowania modeli jest WikiArt zawierający ponad 84 tysiące różnych obrazów. Zbiór ten funkcjonuje w otwartym dostępie i jest darmowy do niekomercyjnego użytku.

## Klasyfikacja gatunku obrazu

Każdy z obrazów posiada etykietę wskazującą na gatunek obrazu:

-   0 - Malarstwo abstrakcyjne

-   1 - Pejzaż miejski

-   2 - Malarstwo rodzajowe

-   3 - Ilustracja

-   4 - Pejzaż

-   5 - Malarstwo nagie

-   6 - Portret

-   7 - Malarstwo religijne

-   8 - Szkic

-   9 - Martwa natura

-   10 - Gatunek nieznany

### Pierwszy projekt sieci

Pierwsza testowana przez nas architekutra sieci jest stosunkowo prosta ponieważ składa się jedynie z 4 warstw konwolucyjnych i max poolingowych ze stosunkowo niewielką ilością filtrów.

<details>

<summary>Kod</summary>

```{r}
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu",
                input_shape = c(150, 150, 3)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_dense(units = 11, activation = "softmax")
```

</details>

```{r echo=FALSE}
model
```

Szkolenie w każdej epoce odbywało się na 3200 obrazkach oraz była przeprowadzana walidacja na 1600 obrazkach.

Funkcją optymalizującą był "Adam" oraz zastosowane były następujące callbacki:

-   Zapisywanie modelu po każdej epoce

-   Przerwanie procesu uczenia jeżeli celność nie poprawi się na zbiorze walidacyjnym po 5 epokach

Przy tej strukturze sieć szkoliła się przez 28 epok kończąc proces uczenia przez zastosowanie callbacków ze względu na brak poprawy celności na zbiorze walidacyjnym.

![Pierwsza siec](plot_nn_1.png)

Widać że nastąpiło zjawisko przeuczenia. Najwyższą celnościa na zbiorze walidacyjnym było \~45%.

### Drugi projekt sieci

Druga architektura składa się z większej liczby warstw konwolucyjnych oraz większej ilości filtrów. Dodatkowo tym razem dodaliśmy warstwy dropout'ów w celu redukcji przeuczenia sieci.

<details>

<summary>Kod</summary>

```{r}
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu",
                input_shape = c(150, 150, 3)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.3) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.3) %>% 
  layer_flatten() %>%
  layer_dense(units = 256, activation = "relu") %>%
  layer_dense(units = 54, activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 11, activation = "softmax")
```

</details>

```{r echo=FALSE}
model
```

Uczenie tym razem odbywało się na 51 200 obrazach w epoce, a następnie model walidowany był na 12 800 obrazach.

Funkcją optymalizującą również był "Adam" oraz zastosowane były te same callbacki co przy wcześniejszej architekturze.

![Druga siec](plot_nn_2.png)

Niestety także i przy tej architekturze nastąpiło zjawisko przeuczenia (nawet szybciej niż w przypadku sieci poprzedniej) osiągając accuracy maksymalnie na poziomie 0.45.

### Trzeci projekt sieci

Tym razem zdecydowaliśmy się użyć wcześniej przeszkolonej sieci ResNet jako części konwolucyjnej. Ze względu na ograniczone zasoby obliczeniowe użyliśmy wersji ResNet50.

```{r}
conv_base <- application_resnet50(
  weights = "imagenet",
  include_top = F,
  input_shape = c(224,224,3)
)
```

```{r}
freeze_weights(conv_base)
unfreeze_weights(conv_base, from = "conv5_block3_2_conv")
```

Odmrażamy dwa ostatnie bloki konwolucyjne i normalizujące partię w celu dostrojenia sieci do naszego zadania.

<details>

<summary>Kod</summary>

```{r}
model <- keras_model_sequential() %>%
  conv_base %>%
  layer_flatten() %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_dense(units = 11, activation = "softmax")
```

</details>

```{r echo=FALSE}
model
```

Sieć uczona była na pełnym zbiorze treningowym przy użyciu optimizera "Adam" i tych samch callbacków co przy poprzednich architekturach.

![Trzecia siec](plot_nn_3.png)

W tym przypadku zjawisko przeuczenia nastąpiło nawet szybiciej niż przy poprzednich architekturach.

### Czwarty projekt sieci

<details>

<summary>Kod</summary>

```{r}
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3),
                input_shape = c(224, 224, 3)) %>%
  layer_activation_leaky_relu(alpha = 0.1) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3)) %>%
  layer_activation_leaky_relu(alpha = 0.1) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.2) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3)) %>%
  layer_activation_leaky_relu(alpha = 0.1) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.3) %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3, 3)) %>%
  layer_activation_leaky_relu(alpha = 0.1) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 512, activation = "relu") %>%
  layer_activity_regularization(l2 = 0.001) %>% 
  layer_dense(units = 11, activation = "softmax")
```

</details>

```{r echo=FALSE}
model
```

Ze względu na fakt że pierwsza architektura pomimo najprostrzej struktury dawała relatywnie najlepsze wyniki zdecydowaliśmy się do niej wrócić z modyfikacjami w postaci regurlaryzacji oraz funkcji aktywacji w postaci "leaky reLU".

![Czwarta siec](plot_nn_4.png)

Podczas uczenia callbacki ustawione były na:

-   Zapisywanie modelu po każdej epoce

-   Przerwanie procesu uczenia jeżeli funkcja straty nie poprawi się na zbiorze walidacyjnym po 5 epokach

-   Obniżenie learning rate jeżeli funkcja straty nie poprawi się na zbiorze walidacyjnym po 3 epokach

W porównaniu do wszystkich poprzednich struktur ta wypada najlepiej zarówno na zbiorze uczącym (~67% celności) jak i walidacyjnym (~47% celności).

Niestety pomimo lepszych wyników uczenia oraz obniżenia learning rate (widać że za pierwszym razem poprawił celność) model jest przeuczony.

### Piąty projekt sieci

Zachęceni wynikami uzyskanymi na stylach z szkolonej od początku architektury DensNet121 postawnowiliśmy użyć jej również do klasyfikacji garunków.

```{r}
conv_base <- application_densenet121(
  weights = NULL,
  include_top = TRUE,
  classes = 11,
  input_shape = c(224, 224, 3)
)

model <- conv_base
```

![Piata siec](plot_nn_5.png)

Także i w tym przypadku architektura ta okazała się najlepsza z dotychczasowych osiągając blisko 60% na zbiorze walidacyjnym.

### Szósty projekt sieci

## Klasyfikacja stylu obrazu

-   0 - Abstrakcjonizm ekspresyjny

-   1 - Malowidło akcji

-   2 - Kubizm anlityczny

-   3 - Secesja

-   4 - Barok

-   5 - Malarstwo barwnych płaszczyzn

-   6 - Realizm współczesny

-   7 - Kubizm

-   8 - Wczesny Renesans

-   9 - Ekspresjonizm

-   10 - Fowizm

-   11 - Późny renesans

-   12 - Impresjonizm

-   13 - Manieryzm późnego renesansu

-   14 - Minimalizm

-   15 - Prymitywizm (Naïve art)

-   16 - Nowy realizm

-   17 - Renesans północny

-   18 - Puentylizm

-   19 - Pop-art

-   20 - Postimpresjonizm

-   21 - Realizm

-   22 - Rokoko

-   23 - Romantyzm

-   24 - Symbolizm

-   25 - Kubizm syntetyczny

-   26 - Ukiyo-e

-   27 - Styl nieznany

### Pierwszy projekt sieci

Pierwszą architekturę sieci do rozpoznawania styli malarstwa postanowiliśmy oprzeć o wcześniej wyuczoną sieć ResNet50 z odmrożonymi dwoma blokami konwolucji.

W tym przypadku zdecydowaliśmy się na mniejszy blok gęsty względem architektury do rozpoznawania gatunków.

<details>

<summary>Kod</summary>

```{r}
model <- keras_model_sequential() %>%
  conv_base %>%
  layer_flatten() %>%
  layer_dense(units = 256, activation = "relu") %>%
  layer_dense(units = 27, activation = "softmax")
```

</details>

```{r echo=FALSE}
model
```

Podczas uczenia callbacki ustawione były na:

-   Zapisywanie modelu po każdej epoce

-   Przerwanie procesu uczenia jeżeli funkcja straty nie poprawi się na zbiorze walidacyjnym po 8 epokach

-   Obniżenie learning rate jeżeli funkcja straty nie poprawi się na zbiorze walidacyjnym po 4 epokach

Niestety proces uczenia został przerwany ze względu na limity platformy kaggle (kompilacja notatnika osiągnęła 12h) z tego względu dysponujemy jedynie logami dla rezultatów 27 epoki

loss: 2.1323 - acc: 0.3101 - val_loss: 2.2895 - val_acc: 0.2795 - lr: 1.0000e-06

### Drugi projekt sieci

Jako drugą architekturę sieci postanowiliśmy wykorzystać czwartą architekturę zaproponowaną do klasyfikacji gatunków ze względu na fakt że osiągała ona najlepsze wyniki.

<details>

<summary>Kod</summary>

```{r}
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3),
                input_shape = c(224, 224, 3)) %>%
  layer_activation_leaky_relu(alpha = 0.1) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3)) %>%
  layer_activation_leaky_relu(alpha = 0.1) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.2) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3)) %>%
  layer_activation_leaky_relu(alpha = 0.1) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.3) %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3, 3)) %>%
  layer_activation_leaky_relu(alpha = 0.1) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 512, activation = "relu") %>%
  layer_activity_regularization(l2 = 0.001) %>% 
  layer_dense(units = 27, activation = "softmax")
```

</details>

```{r echo=FALSE}
model
```


![Druga siec](plot_nn_style_1.png)

Pomimo faktu, że i przy tej strukturze sieci następuje przeuczenie osiąga ona taki sam wynik na zbiorze uczącym po 3 epokach co struktura oparta na ResNet50 osiągnęła po 27.

### Trzeci projekt sieci

Pomimo faktu że modele które wykorzystywały wcześniej przetrenowaną sieć konwolucyjną ResNet50 okazały się być gorsze nawet od prostych struktur zdecydowaliśmy się użyć nowszej struktury DensNet121 wykorzystującej bardziej zaawansowane połączenia resztowe.

![ResNet vs DensNet](densnet_vs_resnet.png)

Zdecydowaliśmy się na DensNet121 z tego względu iż jest to najmniejsza struktura z architektur DensNet co jest ważne przy ograniczonych zasobach sprzętowych.

Tym razem uznaliśmy jednak, że abstrakcyjny charakter zadania wymaga przeszkolenia całej struktury na nowo, ponieważ wzorce wyuczone na zbiorze "Imagenet" mogą nie dawać zadowalających wyników.

```{r}
conv_base <- application_densenet121(
  weights = NULL,
  include_top = TRUE,
  classes = 11,
  input_shape = c(224, 224, 3)
)

model <- conv_base
```

![Trzecia siec](plot_nn_style_2.png)

Decyzja ta okazała się być trafiona. Po raz pierwszy udało się osiągnąć celnośc powyżej 50% (~52%).